{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# HR Persona Bangladesh - Fine-tune Llama 3.2 3B Instruct\n",
        "\n",
        "This notebook fine-tunes Llama 3.2 3B Instruct on Bangladesh Labour Act QA dataset using Unsloth.\n",
        "\n",
        "**Features:**\n",
        "- 2x faster training with Unsloth\n",
        "- 70% less memory usage\n",
        "- 4-bit quantization for free Colab T4 GPU\n",
        "- Export to GGUF Q4_K_M for Ollama\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with T4 GPU (free tier)\n",
        "- Training dataset in ChatML format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_unsloth"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth for 2x faster training (latest version)\n",
        "# This installs the latest stable version from pip + latest updates from GitHub\n",
        "!pip install unsloth\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install other dependencies\n",
        "!pip install datasets huggingface_hub trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## 2. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model_code"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048  # Context length\n",
        "dtype = None  # Auto-detect (Float16 for T4)\n",
        "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
        "\n",
        "# Load Llama 3.2 3B Instruct\n",
        "# Available options:\n",
        "#   - \"unsloth/Llama-3.2-3B-Instruct\" (standard)\n",
        "#   - \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" (pre-quantized, faster loading)\n",
        "#   - \"meta-llama/Llama-3.2-3B-Instruct\" (official, requires HF token)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model.config._name_or_path}\")\n",
        "print(f\"Max sequence length: {max_seq_length}\")\n",
        "print(f\"4-bit quantization: {load_in_4bit}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_config"
      },
      "source": [
        "## 3. Configure LoRA for Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lora_config_code"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank - higher = more capacity but slower\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,  # Scaling factor\n",
        "    lora_dropout=0,  # No dropout for faster training\n",
        "    bias=\"none\",  # No bias for faster training\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 30% less memory\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"LoRA configuration applied!\")\n",
        "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "## 4. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_dataset"
      },
      "outputs": [],
      "source": [
        "# Option 1: Upload your dataset file\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "print(\"Upload your training dataset (JSON file in ChatML format):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "dataset_filename = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {dataset_filename}\")\n",
        "\n",
        "# Load the dataset\n",
        "with open(dataset_filename, 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataset"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, CHAT_TEMPLATES\n",
        "\n",
        "# Check available chat templates\n",
        "print(\"Available chat templates:\", list(CHAT_TEMPLATES.keys()))\n",
        "\n",
        "# Apply Llama 3.2 chat template\n",
        "# Options: \"llama-3.2\", \"llama-32\", \"llama-3\", \"llama3\"\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.2\",\n",
        ")\n",
        "print(\"Applied 'llama-3.2' chat template\")\n",
        "\n",
        "# Convert data to proper format\n",
        "def prepare_data(samples):\n",
        "    \"\"\"Prepare data for training.\"\"\"\n",
        "    formatted = []\n",
        "    \n",
        "    for sample in samples:\n",
        "        # Handle ChatML format\n",
        "        if 'messages' in sample:\n",
        "            messages = sample['messages']\n",
        "        # Handle ShareGPT format\n",
        "        elif 'conversations' in sample:\n",
        "            messages = []\n",
        "            for conv in sample['conversations']:\n",
        "                role = 'user' if conv['from'] == 'human' else 'assistant'\n",
        "                messages.append({'role': role, 'content': conv['value']})\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        formatted.append({'conversations': messages})\n",
        "    \n",
        "    return formatted\n",
        "\n",
        "# Prepare the data\n",
        "prepared_data = prepare_data(raw_data)\n",
        "print(f\"Prepared {len(prepared_data)} conversations\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = Dataset.from_list(prepared_data)\n",
        "\n",
        "# Apply chat template formatting\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = []\n",
        "    for convo in convos:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            convo,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Split into train and validation sets (90% train, 10% validation)\n",
        "dataset_split = dataset.train_test_split(test_size=0.1, seed=3407)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "eval_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(eval_dataset)}\")\n",
        "print(\"\\nSample formatted text:\")\n",
        "print(train_dataset[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 5. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "import math\n",
        "\n",
        "# Training configuration with evaluation\n",
        "# Using SFTConfig (latest TRL/Unsloth pattern)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Add validation dataset\n",
        "    args=SFTConfig(\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,  # Can set to True for shorter sequences\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,  # Batch size for evaluation\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        # num_train_epochs=1,  # Uncomment for full training\n",
        "        max_steps=100,  # Use for quick testing, comment out for full training\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",  # Evaluate during training\n",
        "        eval_steps=25,  # Evaluate every 25 steps\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # Disable wandb\n",
        "        load_best_model_at_end=True,  # Load best model when done\n",
        "        metric_for_best_model=\"eval_loss\",  # Use eval loss to determine best\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Training configuration ready!\")\n",
        "print(f\"Batch size: 2\")\n",
        "print(f\"Gradient accumulation: 4\")\n",
        "print(f\"Effective batch size: 8\")\n",
        "print(f\"Learning rate: 2e-4\")\n",
        "print(f\"Evaluation every: 25 steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 6. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_code"
      },
      "outputs": [],
      "source": [
        "# Check GPU memory before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU: {gpu_stats.name}\")\n",
        "print(f\"GPU Memory: {start_gpu_memory} GB / {max_memory} GB\")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nStarting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Print training stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print('='*50)\n",
        "print(f\"Peak GPU memory: {used_memory} GB\")\n",
        "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "\n",
        "# Final training loss\n",
        "train_loss = trainer_stats.metrics.get('train_loss', 'N/A')\n",
        "print(f\"\\nFinal Training Loss: {train_loss:.4f}\" if isinstance(train_loss, float) else f\"\\nFinal Training Loss: {train_loss}\")\n",
        "\n",
        "# Run final evaluation\n",
        "print(\"\\nRunning final evaluation on validation set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"EVALUATION METRICS\")\n",
        "print('='*50)\n",
        "print(f\"Eval Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Eval Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "print(f\"Eval Runtime: {eval_results['eval_runtime']:.2f} seconds\")\n",
        "print(f\"Eval Samples/sec: {eval_results['eval_samples_per_second']:.2f}\")\n",
        "\n",
        "# Note about accuracy\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"NOTE ON METRICS\")\n",
        "print('='*50)\n",
        "print(\"For language models, we measure:\")\n",
        "print(\"- Loss: Lower is better (measures prediction error)\")\n",
        "print(\"- Perplexity: Lower is better (exp of loss, measures uncertainty)\")\n",
        "print(\"  * Perplexity < 10: Excellent\")\n",
        "print(\"  * Perplexity 10-50: Good\")  \n",
        "print(\"  * Perplexity 50-100: Acceptable\")\n",
        "print(\"  * Perplexity > 100: Needs more training/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test"
      },
      "source": [
        "## 7. Test the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_code"
      },
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompts related to Bangladesh Labour Law\n",
        "test_prompts = [\n",
        "    \"What is the maximum working hours per week according to Bangladesh Labour Act?\",\n",
        "    \"How many days of annual leave is an employee entitled to?\",\n",
        "    \"What are the rules for termination of employment in Bangladesh?\",\n",
        "]\n",
        "\n",
        "print(\"Testing fine-tuned model:\\n\")\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    \n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {response.split('assistant')[-1].strip()}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## 8. Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_lora"
      },
      "outputs": [],
      "source": [
        "# Save LoRA adapters (small ~100MB file)\n",
        "model.save_pretrained(\"hr-persona-bd-llama32-3b-lora\")\n",
        "tokenizer.save_pretrained(\"hr-persona-bd-llama32-3b-lora\")\n",
        "print(\"LoRA adapters saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_merged"
      },
      "outputs": [],
      "source": [
        "# Save merged model in HuggingFace format (optional, larger)\n",
        "# Uncomment if you need the full merged model\n",
        "\n",
        "# model.save_pretrained_merged(\n",
        "#     \"hr-persona-bd-llama32-3b-merged\",\n",
        "#     tokenizer,\n",
        "#     save_method=\"merged_16bit\",\n",
        "# )\n",
        "# print(\"Merged model saved in 16-bit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_gguf"
      },
      "source": [
        "## 9. Export to GGUF for Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_gguf_code"
      },
      "outputs": [],
      "source": [
        "# Export to GGUF Q4_K_M (4-bit quantization)\n",
        "print(\"Exporting to GGUF Q4_K_M format...\")\n",
        "print(\"This may take 5-10 minutes...\\n\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    \"hr-persona-bd-llama32-3b-gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        ")\n",
        "\n",
        "print(\"\\nGGUF export complete!\")\n",
        "# Unsloth often saves GGUF at current dir as llama-3.2-3b-instruct.Q4_K_M.gguf (not inside the folder)\n",
        "# Move it into the output folder so the zip and Modelfile work\n",
        "import os\n",
        "import shutil\n",
        "gguf_at_root = \"llama-3.2-3b-instruct.Q4_K_M.gguf\"\n",
        "out_dir = \"hr-persona-bd-llama32-3b-gguf\"\n",
        "if os.path.isfile(gguf_at_root):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    dest = os.path.join(out_dir, gguf_at_root)\n",
        "    shutil.move(gguf_at_root, dest)\n",
        "    print(\"File saved:\", dest)\n",
        "else:\n",
        "    inside = os.path.join(out_dir, gguf_at_root)\n",
        "    if os.path.isfile(inside):\n",
        "        print(\"File saved:\", inside)\n",
        "    else:\n",
        "        print(\"Check current dir or\", out_dir, \"for *.gguf\")\n",
        "        for f in os.listdir(\".\"):\n",
        "            if f.endswith(\".gguf\"):\n",
        "                print(\"  At root:\", f)\n",
        "        if os.path.isdir(out_dir):\n",
        "            for f in os.listdir(out_dir):\n",
        "                if f.endswith(\".gguf\"):\n",
        "                    print(\"  In folder:\", f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## 10. Download the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_code"
      },
      "outputs": [],
      "source": [
        "# Zip and download the GGUF model\n",
        "import os\n",
        "\n",
        "# List exported files\n",
        "print(\"Exported files:\")\n",
        "for f in os.listdir(\"hr-persona-bd-llama32-3b-gguf\"):\n",
        "    size = os.path.getsize(f\"hr-persona-bd-llama32-3b-gguf/{f}\") / (1024*1024)\n",
        "    print(f\"  {f}: {size:.1f} MB\")\n",
        "\n",
        "# Create zip for download\n",
        "# !zip -r hr-persona-bd-llama32-3b-gguf.zip hr-persona-bd-llama32-3b-gguf/\n",
        "\n",
        "# print(\"\\nDownload the model:\")\n",
        "# files.download(\"hr-persona-bd-llama32-3b-gguf.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Drive (run once)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the fine-tuning directory if it doesn't exist\n",
        "!mkdir -p /content/drive/MyDrive/fine-tuning\n",
        "\n",
        "# Copy GGUF directory to Drive\n",
        "!cp -r hr-persona-bd-llama32-3b-gguf /content/drive/MyDrive/fine-tuning/\n",
        "\n",
        "# Optional: LoRA directory too\n",
        "!cp -r hr-persona-bd-llama32-3b-lora /content/drive/MyDrive/fine-tuning/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_lora"
      },
      "outputs": [],
      "source": [
        "# Also download LoRA adapters (smaller, for further training)\n",
        "!zip -r hr-persona-bd-llama32-3b-lora.zip hr-persona-bd-llama32-3b-lora/\n",
        "\n",
        "print(\"Download LoRA adapters:\")\n",
        "files.download(\"hr-persona-bd-llama32-3b-lora.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollama_instructions"
      },
      "source": [
        "## 11. Using with Ollama\n",
        "\n",
        "After downloading the GGUF model, follow these steps to use it with Ollama:\n",
        "\n",
        "### Step 1: Install Ollama\n",
        "```bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```\n",
        "\n",
        "### Step 2: Create Modelfile\n",
        "Create a file named `Modelfile` inside the GGUF folder. Use the actual GGUF filename (the export cell moves it to `llama-3.2-3b-instruct.Q4_K_M.gguf` inside the folder):\n",
        "\n",
        "```\n",
        "FROM ./llama-3.2-3b-instruct.Q4_K_M.gguf\n",
        "\n",
        "TEMPLATE \"\"\"{{- if .System }}<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ .System }}<|eot_id|>{{- end }}<|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ .Response }}<|eot_id|>\"\"\"\n",
        "\n",
        "SYSTEM \"\"\"You are an expert HR consultant specializing in Bangladesh Labour Law. You have comprehensive knowledge of the Bangladesh Labour Act 2006 and its amendments. Provide accurate, professional advice to HR practitioners.\"\"\"\n",
        "\n",
        "PARAMETER temperature 0.7\n",
        "PARAMETER top_p 0.9\n",
        "PARAMETER stop \"<|eot_id|>\"\n",
        "```\n",
        "\n",
        "### Step 3: Create Ollama Model\n",
        "```bash\n",
        "cd hr-persona-bd-llama32-3b-gguf\n",
        "ollama create hr-persona-bd-llama -f Modelfile\n",
        "```\n",
        "\n",
        "### Step 4: Run the Model\n",
        "```bash\n",
        "ollama run hr-persona-bd-llama\n",
        "```\n",
        "\n",
        "### Step 5: Use via API\n",
        "```bash\n",
        "curl http://localhost:11434/api/chat -d '{\n",
        "  \"model\": \"hr-persona-bd-llama\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"What is the notice period for termination?\"}\n",
        "  ]\n",
        "}'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf_upload"
      },
      "source": [
        "## 12. (Optional) Upload to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf_upload_code"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# UPLOAD TO HUGGING FACE HUB\n",
        "# ============================================\n",
        "# \n",
        "# STEP 1: Get your Hugging Face token from:\n",
        "#         https://huggingface.co/settings/tokens\n",
        "#         (Create a new token with \"Write\" permission)\n",
        "#\n",
        "# STEP 2: Replace \"YOUR_USERNAME\" with your HuggingFace username\n",
        "#         Replace \"YOUR_TOKEN\" with your token\n",
        "#\n",
        "# STEP 3: Uncomment and run the code below\n",
        "# ============================================\n",
        "\n",
        "# --- Configuration (EDIT THESE) ---\n",
        "HF_USERNAME = \"YOUR_USERNAME\"  # e.g., \"john-doe\"\n",
        "HF_TOKEN = \"YOUR_TOKEN\"        # e.g., \"hf_xxxxxxxxxxxx\"\n",
        "# ----------------------------------\n",
        "\n",
        "# Uncomment below to upload:\n",
        "\n",
        "# from huggingface_hub import login\n",
        "# login(token=HF_TOKEN)\n",
        "# print(f\"Logged in as: {HF_USERNAME}\")\n",
        "\n",
        "# # Upload LoRA adapters (small, ~100MB)\n",
        "# print(\"\\n1. Uploading LoRA adapters...\")\n",
        "# model.push_to_hub(f\"{HF_USERNAME}/hr-persona-bd-llama32-3b-lora\")\n",
        "# tokenizer.push_to_hub(f\"{HF_USERNAME}/hr-persona-bd-llama32-3b-lora\")\n",
        "# print(f\"   ✓ LoRA: https://huggingface.co/{HF_USERNAME}/hr-persona-bd-llama32-3b-lora\")\n",
        "\n",
        "# # Upload GGUF model (for Ollama, ~2GB)\n",
        "# print(\"\\n2. Uploading GGUF model...\")\n",
        "# model.push_to_hub_gguf(\n",
        "#     f\"{HF_USERNAME}/hr-persona-bd-llama32-3b-gguf\",\n",
        "#     tokenizer,\n",
        "#     quantization_method=\"q4_k_m\",\n",
        "# )\n",
        "# print(f\"   ✓ GGUF: https://huggingface.co/{HF_USERNAME}/hr-persona-bd-llama32-3b-gguf\")\n",
        "\n",
        "# print(\"\\n✓ All uploads complete!\")\n",
        "\n",
        "print(\"To upload to Hugging Face:\")\n",
        "print(\"1. Get token from: https://huggingface.co/settings/tokens\")\n",
        "print(\"2. Edit HF_USERNAME and HF_TOKEN above\")\n",
        "print(\"3. Uncomment the upload code and run this cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# UPLOAD TRAINING DATASET TO HUGGING FACE\n",
        "# ============================================\n",
        "# This uploads your training dataset so others can use it\n",
        "# or you can reference it in future training runs.\n",
        "# ============================================\n",
        "\n",
        "# Uncomment to upload dataset:\n",
        "\n",
        "# from datasets import Dataset\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # Login (skip if already logged in above)\n",
        "# # login(token=HF_TOKEN)\n",
        "\n",
        "# # Upload the dataset used for training\n",
        "# print(\"Uploading training dataset...\")\n",
        "\n",
        "# # The dataset is already in memory from training\n",
        "# train_dataset.push_to_hub(\n",
        "#     f\"{HF_USERNAME}/hr-persona-bd-labour-act-dataset\",\n",
        "#     private=False,  # Set to True for private dataset\n",
        "#     commit_message=\"Upload Bangladesh Labour Act QA dataset\"\n",
        "# )\n",
        "\n",
        "# print(f\"✓ Dataset: https://huggingface.co/datasets/{HF_USERNAME}/hr-persona-bd-labour-act-dataset\")\n",
        "\n",
        "print(\"To upload dataset: uncomment the code above (after setting HF_USERNAME)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
